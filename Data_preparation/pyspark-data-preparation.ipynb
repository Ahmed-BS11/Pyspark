{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install  pyspark","metadata":{"execution":{"iopub.status.busy":"2023-10-03T17:31:35.044023Z","iopub.execute_input":"2023-10-03T17:31:35.044378Z","iopub.status.idle":"2023-10-03T17:32:24.283303Z","shell.execute_reply.started":"2023-10-03T17:31:35.044350Z","shell.execute_reply":"2023-10-03T17:32:24.282237Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425350 sha256=c3a3a7be3bd96725ac73f252f53b716a7dd0e8be0dbf95ac0aabd457ffb4b356\n  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum\nfrom pyspark.sql.functions import *\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T18:10:38.250592Z","iopub.execute_input":"2023-10-03T18:10:38.251022Z","iopub.status.idle":"2023-10-03T18:10:38.256446Z","shell.execute_reply.started":"2023-10-03T18:10:38.250994Z","shell.execute_reply":"2023-10-03T18:10:38.255205Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder \\\n    .master('local[1]') \\\n    .appName(\"Data Preparation with PySpark\") \\\n    .getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T17:34:56.275975Z","iopub.execute_input":"2023-10-03T17:34:56.276365Z","iopub.status.idle":"2023-10-03T17:35:01.957165Z","shell.execute_reply.started":"2023-10-03T17:34:56.276335Z","shell.execute_reply":"2023-10-03T17:35:01.955931Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/10/03 17:34:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"df=spark.read.csv(\"/kaggle/input/new-york-city-airbnb-open-data/AB_NYC_2019.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-03T17:35:06.534798Z","iopub.execute_input":"2023-10-03T17:35:06.535195Z","iopub.status.idle":"2023-10-03T17:35:13.386106Z","shell.execute_reply.started":"2023-10-03T17:35:06.535163Z","shell.execute_reply":"2023-10-03T17:35:13.384854Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df.show(5)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T17:36:01.530463Z","iopub.execute_input":"2023-10-03T17:36:01.530822Z","iopub.status.idle":"2023-10-03T17:36:01.767459Z","shell.execute_reply.started":"2023-10-03T17:36:01.530792Z","shell.execute_reply":"2023-10-03T17:36:01.766571Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"+----+--------------------+-------+-----------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+--------------------+----------------+\n| _c0|                 _c1|    _c2|        _c3|                _c4|          _c5|     _c6|      _c7|            _c8|  _c9|          _c10|             _c11|       _c12|             _c13|                _c14|            _c15|\n+----+--------------------+-------+-----------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+--------------------+----------------+\n|  id|                name|host_id|  host_name|neighbourhood_group|neighbourhood|latitude|longitude|      room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_l...|availability_365|\n|2539|Clean & quiet apt...|   2787|       John|           Brooklyn|   Kensington|40.64749|-73.97237|   Private room|  149|             1|                9| 2018-10-19|             0.21|                   6|             365|\n|2595|Skylit Midtown Ca...|   2845|   Jennifer|          Manhattan|      Midtown|40.75362|-73.98377|Entire home/apt|  225|             1|               45| 2019-05-21|             0.38|                   2|             355|\n|3647|THE VILLAGE OF HA...|   4632|  Elisabeth|          Manhattan|       Harlem|40.80902| -73.9419|   Private room|  150|             3|                0|       NULL|             NULL|                   1|             365|\n|3831|Cozy Entire Floor...|   4869|LisaRoxanne|           Brooklyn| Clinton Hill|40.68514|-73.95976|Entire home/apt|   89|             1|              270| 2019-07-05|             4.64|                   1|             194|\n+----+--------------------+-------+-----------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+--------------------+----------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df.describe()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T17:39:13.965434Z","iopub.execute_input":"2023-10-03T17:39:13.965815Z","iopub.status.idle":"2023-10-03T17:39:14.072218Z","shell.execute_reply.started":"2023-10-03T17:39:13.965789Z","shell.execute_reply":"2023-10-03T17:39:14.071135Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"DataFrame[summary: string, _c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string]"},"metadata":{}}]},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T17:35:13.387962Z","iopub.execute_input":"2023-10-03T17:35:13.388418Z","iopub.status.idle":"2023-10-03T17:35:13.398625Z","shell.execute_reply.started":"2023-10-03T17:35:13.388360Z","shell.execute_reply":"2023-10-03T17:35:13.397565Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n |-- _c8: string (nullable = true)\n |-- _c9: string (nullable = true)\n |-- _c10: string (nullable = true)\n |-- _c11: string (nullable = true)\n |-- _c12: string (nullable = true)\n |-- _c13: string (nullable = true)\n |-- _c14: string (nullable = true)\n |-- _c15: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"row_count = df.count()\n\nprint(\"Number of rows:\", row_count)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T17:50:59.790646Z","iopub.execute_input":"2023-10-03T17:50:59.791057Z","iopub.status.idle":"2023-10-03T17:50:59.926323Z","shell.execute_reply.started":"2023-10-03T17:50:59.791028Z","shell.execute_reply":"2023-10-03T17:50:59.925218Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Number of rows: 49080\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Check For Missing Values** \n\nPourcentage of missing value in each column","metadata":{}},{"cell_type":"code","source":"missing_counts = df.agg(*[\n    (1 - (sum(col(c).isNotNull().cast(\"int\")) / df.count())).alias(c)\n    for c in df.columns\n])\n\nmissing_counts.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T17:45:32.234802Z","iopub.execute_input":"2023-10-03T17:45:32.235164Z","iopub.status.idle":"2023-10-03T17:45:36.469631Z","shell.execute_reply.started":"2023-10-03T17:45:32.235138Z","shell.execute_reply":"2023-10-03T17:45:36.468554Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+--------------------+--------------------+\n|_c0|                 _c1|                 _c2|                 _c3|                 _c4|                 _c5|                 _c6|                 _c7|                 _c8|                 _c9|                _c10|                _c11|              _c12|              _c13|                _c14|                _c15|\n+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+--------------------+--------------------+\n|0.0|6.519967400162585E-4|0.003769356153219272|0.004197229013854886|0.003769356153219272|0.003769356153219272|0.003769356153219272|0.003769356153219272|0.003769356153219272|0.003769356153219272|0.003769356153219272|0.004176854115729434|0.2085167074164629|0.2081295843520783|0.003810105949470...|0.006968215158924207|\n+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+--------------------+--------------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Missing Values per column**","metadata":{}},{"cell_type":"code","source":"columns = df.columns\n\n# Filtre is a transformation\nnull_counts = [df.filter(col(c).isNull()).count() for c in columns]\n\n# Create a DataFrame to display the results\nnull_counts_df = spark.createDataFrame([(columns[i], null_counts[i]) for i in range(len(columns))], [\"Column\", \"Null Count\"])\n\nnull_counts_df.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T17:47:31.389286Z","iopub.execute_input":"2023-10-03T17:47:31.389687Z","iopub.status.idle":"2023-10-03T17:47:36.818994Z","shell.execute_reply.started":"2023-10-03T17:47:31.389658Z","shell.execute_reply":"2023-10-03T17:47:36.817830Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"[Stage 112:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+------+----------+\n|Column|Null Count|\n+------+----------+\n|   _c0|         0|\n|   _c1|        32|\n|   _c2|       185|\n|   _c3|       206|\n|   _c4|       185|\n|   _c5|       185|\n|   _c6|       185|\n|   _c7|       185|\n|   _c8|       185|\n|   _c9|       185|\n|  _c10|       185|\n|  _c11|       205|\n|  _c12|     10234|\n|  _c13|     10215|\n|  _c14|       187|\n|  _c15|       342|\n+------+----------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"**Since we have 49,080 rows, we can drop the rows with a nan count of under 500. However, for columns like c12 and c13, we need to apply a different technique(replace by the mean).**","metadata":{}},{"cell_type":"code","source":"print(df.columns)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T18:10:14.704628Z","iopub.execute_input":"2023-10-03T18:10:14.705008Z","iopub.status.idle":"2023-10-03T18:10:14.711112Z","shell.execute_reply.started":"2023-10-03T18:10:14.704981Z","shell.execute_reply":"2023-10-03T18:10:14.709901Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13', '_c14', '_c15']\n","output_type":"stream"}]},{"cell_type":"code","source":"columns_to_dropna=['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c14', '_c15']\ndf=df.dropna(subset=columns_to_dropna)\n\nmean_values = df.select(*(mean(col(c)).alias(c) for c in columns_to_fillna)).collect()[0].asDict()#we add [0] because collect return a list\n\n\n# Fill missing values with the calculated means\ndf = df.fillna(mean_values, subset=columns_to_fillna)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T18:17:23.470731Z","iopub.execute_input":"2023-10-03T18:17:23.472197Z","iopub.status.idle":"2023-10-03T18:17:23.986496Z","shell.execute_reply.started":"2023-10-03T18:17:23.472141Z","shell.execute_reply":"2023-10-03T18:17:23.985414Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"columns = df.columns\n\n# Filtre is a transformation\nnull_counts = [df.filter(col(c).isNull()).count() for c in columns]\n\n# Create a DataFrame to display the results\nnull_counts_df = spark.createDataFrame([(columns[i], null_counts[i]) for i in range(len(columns))], [\"Column\", \"Null Count\"])\n\nnull_counts_df.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T18:17:25.809260Z","iopub.execute_input":"2023-10-03T18:17:25.809612Z","iopub.status.idle":"2023-10-03T18:17:29.323928Z","shell.execute_reply.started":"2023-10-03T18:17:25.809585Z","shell.execute_reply":"2023-10-03T18:17:29.323093Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"+------+----------+\n|Column|Null Count|\n+------+----------+\n|   _c0|         0|\n|   _c1|         0|\n|   _c2|         0|\n|   _c3|         0|\n|   _c4|         0|\n|   _c5|         0|\n|   _c6|         0|\n|   _c7|         0|\n|   _c8|         0|\n|   _c9|         0|\n|  _c10|         0|\n|  _c11|         0|\n|  _c12|         0|\n|  _c13|         0|\n|  _c14|         0|\n|  _c15|         0|\n+------+----------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(df.first())","metadata":{"execution":{"iopub.status.busy":"2023-10-03T18:07:19.414819Z","iopub.execute_input":"2023-10-03T18:07:19.415225Z","iopub.status.idle":"2023-10-03T18:07:19.500988Z","shell.execute_reply.started":"2023-10-03T18:07:19.415192Z","shell.execute_reply":"2023-10-03T18:07:19.500219Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Row(_c0='id', _c1='name', _c2='host_id', _c3='host_name', _c4='neighbourhood_group', _c5='neighbourhood', _c6='latitude', _c7='longitude', _c8='room_type', _c9='price', _c10='minimum_nights', _c11='number_of_reviews', _c12='last_review', _c13='reviews_per_month', _c14='calculated_host_listings_count', _c15='availability_365')\n","output_type":"stream"}]},{"cell_type":"code","source":"print(df.head(15000)[1200])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T18:23:16.944778Z","iopub.execute_input":"2023-10-03T18:23:16.945139Z","iopub.status.idle":"2023-10-03T18:23:17.550304Z","shell.execute_reply.started":"2023-10-03T18:23:16.945103Z","shell.execute_reply":"2023-10-03T18:23:17.549303Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Row(_c0='512775', _c1='The Cottage / 1500 sqft. of Privacy', _c2='2396295', _c3='Richard', _c4='Queens', _c5='Long Island City', _c6='40.75296', _c7='-73.93716', _c8='Entire home/apt', _c9='350', _c10='2', _c11='182', _c12='2019-06-11', _c13='2.20', _c14='1', _c15='272')\n","output_type":"stream"}]},{"cell_type":"code","source":"\nmode_values = []\n\n# Iterate through each column in the DataFrame\nfor column in df.columns:\n    # Calculate the mode for the current column\n    mode_result = df.groupBy(column).agg(count(column).alias('count')).sort(desc('count')).limit(1)\n    \n    # Get the mode value from the result\n    mode_value = mode_result.collect()[0][0]\n    \n    # Append the column name and mode value to the list\n    mode_values.append((column, mode_value))\n\n# Display the column names and their mode values\nfor column, mode_value in mode_values:\n    print(f\"Column: {column}, Mode: {mode_value}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T18:24:45.105109Z","iopub.execute_input":"2023-10-03T18:24:45.105555Z","iopub.status.idle":"2023-10-03T18:24:57.551708Z","shell.execute_reply.started":"2023-10-03T18:24:45.105521Z","shell.execute_reply":"2023-10-03T18:24:57.550593Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Column: _c0, Mode: 16974\nColumn: _c1, Mode: Hillside Hotel\nColumn: _c2, Mode: 219517861\nColumn: _c3, Mode: Michael\nColumn: _c4, Mode: Manhattan\nColumn: _c5, Mode: Williamsburg\nColumn: _c6, Mode: 40.71813\nColumn: _c7, Mode: -73.95677\nColumn: _c8, Mode: Entire home/apt\nColumn: _c9, Mode: 100\nColumn: _c10, Mode: 1\nColumn: _c11, Mode: 0\nColumn: _c12, Mode: 5.436951428571429\nColumn: _c13, Mode: 1.3697830864293556\nColumn: _c14, Mode: 1\nColumn: _c15, Mode: 0\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}